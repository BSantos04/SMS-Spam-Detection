{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tJUQHJG61p37"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import argparse\n",
        "import chardet as cd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuBCbI3A2HW6",
        "outputId": "d38a46ca-8a42-469d-e298-f66f5bd5a5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Give a glimpse of the raw dataset:\n",
            "\n",
            "  v1                                                                                                                                                          v2 Unnamed: 2 Unnamed: 3 Unnamed: 4\n",
            " ham                                             Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...        NaN        NaN        NaN\n",
            " ham                                                                                                                               Ok lar... Joking wif u oni...        NaN        NaN        NaN\n",
            "spam Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's        NaN        NaN        NaN\n",
            " ham                                                                                                           U dun say so early hor... U c already then say...        NaN        NaN        NaN\n",
            " ham                                                                                               Nah I don't think he goes to usf, he lives around here though        NaN        NaN        NaN\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # GitHub URL of the dataset raw file\n",
        "    url = \"https://github.com/BSantos04/SMS-Spam-Detection/raw/refs/heads/main/datasets/spam.csv\"\n",
        "\n",
        "    # Open the .csv file and detect the type of encoding of the dataset\n",
        "    rawdata = requests.get(url).content\n",
        "    result = cd.detect(rawdata)\n",
        "    encoding = result[\"encoding\"]\n",
        "\n",
        "    # Convert the .csv file into a Pandas dataframe specifying the detected encoding\n",
        "    df = pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "    # Display the first 5 rows of the raw dataframe\n",
        "    print(\"-\"*211)\n",
        "    print(\"Give a glimpse of the raw dataset:\\n\")\n",
        "    print(df.head().to_string(index=False))\n",
        "except Exception as e:\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYnsuiWvOocJ",
        "outputId": "eb02c2bd-3491-4bef-8fe0-6b0b65e1d8fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Give a glimpse of the pre-processed dataset:\n",
            "\n",
            " SPAM/HAM                                                                                                                                                         SMS\n",
            "        1                                             go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...\n",
            "        1                                                                                                                               ok lar... joking wif u oni...\n",
            "        0 free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\n",
            "        1                                                                                                           u dun say so early hor... u c already then say...\n",
            "        1                                                                                               nah i don't think he goes to usf, he lives around here though\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Restrain the dataset to only the first 2 columns, since the other are just filling columns and those are the only that have any relevant data\n",
        "    df = df.loc[:, [\"v1\", \"v2\"]].copy()\n",
        "\n",
        "    # Remove duplicate row\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Since it's crucial that a row has data on both columns, I chose to remove any row with NaN values\n",
        "    df.dropna(axis=0, how=\"any\", inplace=True)\n",
        "\n",
        "    # Rename the columns for a more intuitive work from now on\n",
        "    df.rename(columns={\"v1\": \"SPAM/HAM\", \"v2\": \"SMS\"}, inplace=True)\n",
        "\n",
        "    # Remove additional empty spaces and put the text of every column content to lowercase\n",
        "    df[\"SPAM/HAM\"] = df[\"SPAM/HAM\"].str.lower().str.strip()\n",
        "    df[\"SMS\"] = df[\"SMS\"].str.lower().str.strip()\n",
        "\n",
        "    # Ordinal codification for 'SPAM/HAM' column\n",
        "    ord_spam = {\"spam\": 0, \"ham\": 1}\n",
        "    df[\"SPAM/HAM\"] = df[\"SPAM/HAM\"].map(ord_spam)\n",
        "\n",
        "    # Display the first 5 rows of the now pre-processed dataset\n",
        "    print(\"-\"*211)\n",
        "    print(\"Give a glimpse of the pre-processed dataset:\\n\")\n",
        "    print(df.head().to_string(index=False))\n",
        "except Exception as e:\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhSvBVKkRidP",
        "outputId": "746186f7-a0ba-43fa-83d7-6ad302ae4110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Confusion Matrix:\n",
            " [[ 207   13]\n",
            " [  19 1312]]\n",
            "\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93       220\n",
            "           1       0.99      0.99      0.99      1331\n",
            "\n",
            "    accuracy                           0.98      1551\n",
            "   macro avg       0.95      0.96      0.96      1551\n",
            "weighted avg       0.98      0.98      0.98      1551\n",
            "\n",
            "\n",
            "\n",
            "AUC-ROC Score: 0.9946349293081074\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "The message: Hey johnny, wassup!?\n",
            "\n",
            "The verdict: HAM!!!\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "The message: WINNER!! This is the secret code to get a brand new iPhone: IPHONE4U.\n",
            "\n",
            "The verdict: SPAM!!!\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "The message: It's a me, Mario\n",
            "\n",
            "The verdict: HAM!!!\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Define the dependent and idenpendent variables (being X the independent variable and y the dependent variable)\n",
        "    X = df[\"SMS\"]\n",
        "    y = df[\"SPAM/HAM\"]\n",
        "\n",
        "    # Split the data into training data and test data, setting the test data as 30% of the dataset and training data 70%, with a random state of 42\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Create a scikit-learn pipeline that will vectorize data and apply LOgistic Regression for the training and test set\n",
        "    pipeline = make_pipeline(TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, stop_words=None, min_df=2), LogisticRegression(class_weight=\"balanced\", max_iter=1000))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Get the predictions and probability based on the test datasets\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Evaluate the model using methods such as Confusion Matrix, Rating Report (focusing on F1-Score) and AUC-ROC Score\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred)\n",
        "    auc_roc_score = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "    # Display the results of the evaluations\n",
        "    print(\"-\"*211)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "    print(\"\\n\\nClassification Report:\\n\", class_report)\n",
        "    print(f\"\\n\\nAUC-ROC Score: {auc_roc_score}\")\n",
        "\n",
        "    # Predict if the message is SPAM or not\n",
        "    sms_one = \"Hey johnny, wassup!?\"\n",
        "    sms_two = \"WINNER!! This is the secret code to get a brand new iPhone: IPHONE4U.\"\n",
        "    sms_three = \"It's a me, Mario\"\n",
        "    pred_one = pipeline.predict([sms_one.strip().lower()])\n",
        "    pred_two = pipeline.predict([sms_two.strip().lower()])\n",
        "    pred_three = pipeline.predict([sms_three.strip().lower()])\n",
        "\n",
        "    # Display the results of the predictions\n",
        "    print(\"-\"*211)\n",
        "    print(f\"The message: {sms_one}\")\n",
        "    print(f\"\\nThe verdict: {'SPAM' if pred_one==0 else 'HAM'}!!!\")\n",
        "    print(\"-\"*211)\n",
        "    print(f\"The message: {sms_two}\")\n",
        "    print(f\"\\nThe verdict: {'SPAM' if pred_two==0 else 'HAM'}!!!\")\n",
        "    print(\"-\"*211)\n",
        "    print(f\"The message: {sms_three}\")\n",
        "    print(f\"\\nThe verdict: {'SPAM' if pred_three==0 else 'HAM'}!!!\")\n",
        "    print(\"-\"*211)\n",
        "except Exception as e:\n",
        "    raise e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
